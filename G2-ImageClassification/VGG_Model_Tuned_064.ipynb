{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGG_Model_Tuned_064.ipynb","provenance":[{"file_id":"1vs2cZZKugm7n1tJMGRF_mD29HPGg9x4j","timestamp":1590221648506},{"file_id":"1FtvE3c7Z2dIGGARd3eF-5gEPN0KWYdYf","timestamp":1590221525079},{"file_id":"1V16R1w6GqpZCBHTSy3kLE-Jy8SBejpKz","timestamp":1590220787858},{"file_id":"12h9clf9s6LE5QxZj1lJUd9oJXcctHiLn","timestamp":1590219872306},{"file_id":"1IpxC85pU-1TO4EB1CMZIVuUdmISGDO5S","timestamp":1590219466811},{"file_id":"183RakXCuHlJa1J-stphbq0-F6bEKuFx8","timestamp":1590219251007},{"file_id":"1zw8SoxbLgTObrQW3fLehhuduB32AT9AY","timestamp":1590211732082}],"collapsed_sections":[],"authorship_tag":"ABX9TyNesr6VvxsIygEUN/JUr5zN"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"utMF5R1icw2y","colab_type":"code","colab":{}},"source":["# VGG16 with tuning on the planet dataset\n","import sys\n","from numpy import load\n","from matplotlib import pyplot\n","from sklearn.model_selection import train_test_split\n","from keras import backend\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.optimizers import SGD\n","from keras.applications.vgg16 import VGG16\n","from keras.models import Model\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","# load train and test dataset\n","def load_dataset():\n","\t# load dataset\n","\tdata = load('/team55/planet_data_064.npz')\n","\tX, y = data['arr_0'], data['arr_1']\n","\t# separate into train and test datasets\n","\ttrainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n","\tprint(trainX.shape, trainY.shape, testX.shape, testY.shape)\n","\treturn trainX, trainY, testX, testY\n","\n","# calculate fbeta score for multi-class/label classification\n","def fbeta(y_true, y_pred, beta=2):\n","\t# clip predictions\n","\ty_pred = backend.clip(y_pred, 0, 1)\n","\t# calculate elements\n","\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n","\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n","\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n","\t# calculate precision\n","\tp = tp / (tp + fp + backend.epsilon())\n","\t# calculate recall\n","\tr = tp / (tp + fn + backend.epsilon())\n","\t# calculate fbeta, averaged across each class\n","\tbb = beta ** 2\n","\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n","\treturn fbeta_score\n","\n","# define cnn model\n","def define_model(in_shape=(64, 64, 3), out_shape=17):\n","\t# load model\n","\tmodel = VGG16(include_top=False, input_shape=in_shape)\n","\t# mark loaded layers as not trainable\n","\tfor layer in model.layers:\n","\t\tlayer.trainable = False\n","\t# allow last vgg block to be trainable\n","\tmodel.get_layer('block5_conv1').trainable = True\n","\tmodel.get_layer('block5_conv2').trainable = True\n","\tmodel.get_layer('block5_conv3').trainable = True\n","\tmodel.get_layer('block5_pool').trainable = True\n","\t# add new classifier layers\n","\tflat1 = Flatten()(model.layers[-1].output)\n","\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n","\toutput = Dense(out_shape, activation='sigmoid')(class1)\n","\t# define new model\n","\tmodel = Model(inputs=model.inputs, outputs=output)\n","\t# compile model\n","\topt = SGD(lr=0.01, momentum=0.9)\n","\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n","\treturn model\n","\n","# plot diagnostic learning curves\n","def summarize_diagnostics(history):\n","\t# plot loss\n","\tpyplot.subplot(211)\n","\tpyplot.title('Cross Entropy Loss')\n","\tpyplot.plot(history.history['loss'], color='blue', label='train')\n","\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n","\t# plot accuracy\n","\tpyplot.subplot(212)\n","\tpyplot.title('Fbeta')\n","\tpyplot.plot(history.history['fbeta'], color='blue', label='train')\n","\tpyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n","\t# save plot to file\n","\tfilename = sys.argv[0].split('/')[-1]\n","\tpyplot.savefig('/team55/' + filename + '_plot.png')\n","  pyplot.show()\n","\tpyplot.close()\n","\n","# run the test harness for evaluating a model\n","def run_test_harness():\n","\t# load dataset\n","\ttrainX, trainY, testX, testY = load_dataset()\n","\t# create data generator\n","\tdatagen = ImageDataGenerator(featurewise_center=True)\n","\t# specify imagenet mean values for centering\n","\tdatagen.mean = [123.68, 116.779, 103.939]\n","\t# prepare iterators\n","\ttrain_it = datagen.flow(trainX, trainY, batch_size=64)\n","\ttest_it = datagen.flow(testX, testY, batch_size=64)\n","\t# define model\n","\tmodel = define_model()\n","\t# fit model\n","\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n","\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=20, verbose=0)\n","\t# evaluate model\n","\tloss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n","\tprint('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n","\t# learning curves\n","\tsummarize_diagnostics(history)\n","\n","# entry point, run the test harness\n","run_test_harness()"],"execution_count":0,"outputs":[]}]}