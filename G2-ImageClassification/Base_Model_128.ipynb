{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Base_Model_128.ipynb","provenance":[{"file_id":"1zw8SoxbLgTObrQW3fLehhuduB32AT9AY","timestamp":1590211732082}],"collapsed_sections":[],"authorship_tag":"ABX9TyNXDNQiF/yVp38ra+y+c0JM"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"RQ2OZcm78Imk","colab_type":"code","colab":{}},"source":["# Baseline model for the planet dataset\n","import sys\n","from numpy import load\n","from matplotlib import pyplot\n","from sklearn.model_selection import train_test_split\n","from keras import backend\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers import Conv2D\n","from keras.layers import MaxPooling2D\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.optimizers import SGD\n","\n","# load train and test dataset\n","def load_dataset():\n","\t# load dataset\n","\tdata = load('/team55/planet_data_128.npz')\n","\tX, y = data['arr_0'], data['arr_1']\n","\t# separate into train and test datasets\n","\ttrainX, testX, trainY, testY = train_test_split(X, y, test_size=0.3, random_state=1)\n","\tprint(trainX.shape, trainY.shape, testX.shape, testY.shape)\n","\treturn trainX, trainY, testX, testY\n","\n","# calculate fbeta score for multi-class/label classification\n","def fbeta(y_true, y_pred, beta=2):\n","\t# clip predictions\n","\ty_pred = backend.clip(y_pred, 0, 1)\n","\t# calculate elements\n","\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n","\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n","\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n","\t# calculate precision\n","\tp = tp / (tp + fp + backend.epsilon())\n","\t# calculate recall\n","\tr = tp / (tp + fn + backend.epsilon())\n","\t# calculate fbeta, averaged across each class\n","\tbb = beta ** 2\n","\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n","\treturn fbeta_score, p\n","\n","# define cnn model\n","def define_model(in_shape=(128, 128, 3), out_shape=17):\n","\tmodel = Sequential()\n","\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n","\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n","\tmodel.add(MaxPooling2D((2, 2)))\n","\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n","\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n","\tmodel.add(MaxPooling2D((2, 2)))\n","\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n","\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n","\tmodel.add(MaxPooling2D((2, 2)))\n","\tmodel.add(Flatten())\n","\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n","\tmodel.add(Dense(out_shape, activation='sigmoid'))\n","\t# compile model\n","\topt = SGD(lr=0.01, momentum=0.9)\n","\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n","\treturn model\n","\n","# plot diagnostic learning curves\n","def summarize_diagnostics(history):\n","\t# plot loss\n","\tpyplot.subplot(211)\n","\tpyplot.title('Cross Entropy Loss')\n","\tpyplot.plot(history.history['loss'], color='blue', label='train')\n","\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n","\t# plot accuracy\n","\tpyplot.subplot(212)\n","\tpyplot.title('Fbeta')\n","\tpyplot.plot(history.history['fbeta'], color='blue', label='train')\n","\tpyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n","\t# save plot to file\n","\tfilename = sys.argv[0].split('/')[-1]\n","\tpyplot.savefig('/team55/' + filename + '_plot.png')\n","  pyplot.show()\n","\tpyplot.close()\n","\n","# run the test harness for evaluating a model\n","def run_test_harness():\n","\t# load dataset\n","\ttrainX, trainY, testX, testY = load_dataset()\n","\t# create data generator\n","\tdatagen = ImageDataGenerator(rescale=1.0/255.0)\n","\t# prepare iterators\n","\ttrain_it = datagen.flow(trainX, trainY, batch_size=128)\n","\ttest_it = datagen.flow(testX, testY, batch_size=128)\n","\t# define model\n","\tmodel = define_model()\n","\t# fit model\n","\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n","\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=0)\n","\t# evaluate model\n","\tloss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n","\tprint('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n","\t# learning curves\n","\tsummarize_diagnostics(history)\n","\n","# entry point, run the test harness\n","run_test_harness()"],"execution_count":0,"outputs":[]}]}